{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kickstarter dataset project - Simple training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799494649
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly as plt\n",
        "import numpy as np\n",
        "\n",
        "pd.options.display.max_rows = 4000\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import plotly.express as px \n",
        "import plotly.subplots as tls\n",
        "import plotly\n",
        "import plotly.offline as py\n",
        "from plotly.offline import init_notebook_mode, iplot, plot\n",
        "import plotly.graph_objs as go\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='Q1'></a>\n",
        "\n",
        "# Reading and cleaning up the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### I am omitting this step for the sake of clarity. This part would made us read from the original dataset and cleaning it up + adding relevant features if the original features are not right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Going from the final dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799476447
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('out2.zip', compression='zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799538766
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799539836
        }
      },
      "outputs": [],
      "source": [
        "df = df.drop(['goal', 'pledged', 'usd pledged'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799541472
        }
      },
      "outputs": [],
      "source": [
        "df.describe(include='all', datetime_is_numeric=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799542634
        }
      },
      "outputs": [],
      "source": [
        "# Converting the columns into the right dtypes as for dates and numbers.\n",
        "df[\"deadline\"] = pd.to_datetime(df['deadline'])\n",
        "df[\"launched\"] = pd.to_datetime(df['launched'])\n",
        "df[\"ID\"] = pd.to_numeric(df[\"ID\"])\n",
        "df[\"backers\"] = pd.to_numeric(df[\"backers\"])\n",
        "df[\"real_usd_pledged\"] = pd.to_numeric(df[\"real_usd_pledged\"])\n",
        "df[\"usd_goal\"] = pd.to_numeric(df[\"usd_goal\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799544022
        }
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799544464
        }
      },
      "outputs": [],
      "source": [
        "df.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799544975
        }
      },
      "outputs": [],
      "source": [
        "df[df['country'].isnull()].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799545581
        }
      },
      "outputs": [],
      "source": [
        "df[df['country'].isnull()].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's drop these because we can see that there is 0 backers and no country nor usd pledged previously, it seems to be a mistake in getting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799548810
        }
      },
      "outputs": [],
      "source": [
        "df = df[~df['country'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799549438
        }
      },
      "outputs": [],
      "source": [
        "df = df.loc[~((df['real_usd_pledged']>=df['usd_goal']) & (df['state']=='failed'))].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799551446
        }
      },
      "outputs": [],
      "source": [
        "df.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799552612
        }
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799553960
        }
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799554405
        }
      },
      "outputs": [],
      "source": [
        "counts = df['name'].value_counts().rename_axis('name').reset_index(name='counts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799554770
        }
      },
      "outputs": [],
      "source": [
        "duplicate_names = df[df['name'].isin(counts[counts['counts']>1].name.tolist())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799555301
        }
      },
      "outputs": [],
      "source": [
        "duplicate_names.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799556092
        }
      },
      "outputs": [],
      "source": [
        "duplicate_names.sort_values(by=['name']).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I'll leave it as it is, but it's interesting to see that some duplicates seem genuine, others seem to be about the same project revamped/relaunched and others are also another rendition of the same project (play at theater and video for instance...).\n",
        "\n",
        "It would be interesting to know more about the motives and mindset of people creating these projects 'again' (needs of funds again), are there also possible cases of reboot of past successful projects (hoax ?). \n",
        "\n",
        "Overall, it still can be integrated in our model as we want to predict the success/failure of a campaign regardless."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution of goals and pledges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674792227542
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674792228820
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# # Campaign length\n",
        "# df['campaign_days'] = df['deadline'] - df['launched']\n",
        "# df['campaign_days'] = df['campaign_days'].dt.round('d').dt.days # Rounding to nearest days, then showing as number only\n",
        "\n",
        "# # Launch day of week\n",
        "# df['launch_day'] = df['launched'].dt.day_name()\n",
        "\n",
        "# # Deadline day of week\n",
        "# df['deadline_day'] = df['deadline'].dt.day_name()\n",
        "\n",
        "# # Launch month\n",
        "# df['launch_month'] = df['launched'].dt.month_name()\n",
        "\n",
        "# # Deadline month\n",
        "# df['deadline_month'] = df['deadline'].dt.month_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799563868
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.groupby('main_category').main_category.count().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674792759358
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "# Creating a dataframe grouped by category with columns for failed and successful\n",
        "cat_df = pd.get_dummies(df.set_index('main_category').state).groupby('main_category').sum()\n",
        "# Plotting\n",
        "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(12,12))\n",
        "color = cm.CMRmap(np.linspace(0.1,0.8,df.main_category.nunique()))\n",
        "\n",
        "df.groupby('main_category').category.count().plot(kind='bar', ax=ax1, color=color)\n",
        "ax1.set_title('Number of projects')\n",
        "ax1.set_xlabel('')\n",
        "df.groupby('main_category').usd_goal.median().plot(kind='bar', ax=ax2, color=color)\n",
        "ax2.set_title('Median project goal ($)')\n",
        "ax2.set_xlabel('')\n",
        "df.groupby('main_category').real_usd_pledged.median().plot(kind='bar', ax=ax3, color=color)\n",
        "ax3.set_title('Median pledged per project ($)')\n",
        "ax3.set_xlabel('')\n",
        "cat_df.div(cat_df.sum(axis=1), axis=0).successful.plot(kind='bar', ax=ax4, color=color) # Normalizes counts across rows\n",
        "ax4.set_title('Proportion of successful projects')\n",
        "ax4.set_xlabel('')\n",
        "df.groupby('main_category').backers.median().plot(kind='bar', ax=ax5, color=color)\n",
        "ax5.set_title('Median backers per project')\n",
        "ax5.set_xlabel('')\n",
        "fig.subplots_adjust(hspace=0.6)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674792994153
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Checking the distributions of continuous features\n",
        "df[df.describe().columns].hist(figsize=(12,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We take the log to better see the distributions as we have outliers in both cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674789101963
        }
      },
      "outputs": [],
      "source": [
        "df_failed = df[df[\"state\"] == \"failed\"]\n",
        "df_sucess = df[df[\"state\"] == \"successful\"]\n",
        "\n",
        "\n",
        "# Add histogram data\n",
        "failed = np.log(df_failed['usd_goal']+1)\n",
        "success = np.log(df_sucess['usd_goal']+1)\n",
        "\n",
        "trace1 = go.Histogram(\n",
        "    x=failed,\n",
        "    opacity=0.60, nbinsx=30, name='Goals Failed', histnorm='probability'\n",
        ")\n",
        "trace2 = go.Histogram(\n",
        "    x=success,\n",
        "    opacity=0.60, nbinsx=30, name='Goals Sucessful', histnorm='probability'\n",
        ")\n",
        "\n",
        "data = [trace1, trace2]\n",
        "layout = go.Layout(barmode='overlay', title=go.layout.Title(text=\"Distributions of usd_goal\"))\n",
        "\n",
        "fig = go.Figure(\n",
        "    data=data,\n",
        "    layout=layout\n",
        ")\n",
        "\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the above histogram, it seems the failed projects tend to have higher values (so higher goals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674789105213
        }
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "fig = px.box(df, x=\"main_category\", y=\"usd_goal\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature engineering\n",
        "\n",
        "Variables for the logistic regression:\n",
        "* len(name) to take into account the name of the project\n",
        "* if the name has all upper case words\n",
        "* if the name contains ! or ?\n",
        "* number of words in name\n",
        "* does the name contains non alphanumeric characters\n",
        "* duration between launch and deadline\n",
        "* month of launch_date\n",
        "\n",
        "Others \n",
        "\n",
        "* goal in usd\n",
        "* category (1-hot encoded)\n",
        "* main category (1-hot encoded)\n",
        "* country (1-hot encoding)\n",
        "\n",
        "to predict target variable state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799583188
        }
      },
      "outputs": [],
      "source": [
        "def getDelta(a,b):\n",
        "    '''Get diffence in days between launch and deadline'''\n",
        "    return (a - b).days\n",
        "\n",
        "# Duration of the project   \n",
        "df['duration'] = df.apply(lambda x: getDelta(x['deadline'],x['launched']),axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799584432
        }
      },
      "outputs": [],
      "source": [
        "df['month'] = df['launched'].dt.month\n",
        "df['year_month'] = df['launched'].map(lambda x: str(x.year) + \"-\" + str(x.month))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799585012
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def has_non_chars(name):\n",
        "    for c in name:\n",
        "        if not c.isalpha() and c!='?' and c!='!':\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def has_exclamation_interrogation(name):\n",
        "    if (\"!\" in name or \"?\" in name):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "def has_upper(name):\n",
        "    for word in name.split(' '):\n",
        "        if word.isupper() and len(re.sub(r'\\W+', '', word))>1:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799585569
        }
      },
      "outputs": [],
      "source": [
        "df['len_name'] = df.name.str.len()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799586142
        }
      },
      "outputs": [],
      "source": [
        "df['name_nb_words'] = df.name.apply(lambda x: len(str(x).split(' ')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799586699
        }
      },
      "outputs": [],
      "source": [
        "df['name_non_chars'] = df.name.apply(has_non_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799587250
        }
      },
      "outputs": [],
      "source": [
        "df['name_has_symbol'] = df.name.apply(has_exclamation_interrogation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799587826
        }
      },
      "outputs": [],
      "source": [
        "df['name_upper'] = df.name.apply(has_upper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799589272
        }
      },
      "outputs": [],
      "source": [
        "df['cat_full'] = df[[\"main_category\",\"category\"]].agg('-'.join, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799589852
        }
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='Q2'></a>\n",
        "\n",
        "## I. Let's prepare the dataset to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799614780
        }
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799631211
        }
      },
      "outputs": [],
      "source": [
        "ks = df.drop(['ID','name','deadline','launched','year_month', 'backers', 'real_usd_pledged'], axis=1).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799632384
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ks.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "usd_goal is skewed, let's check the distribution here, let's replace it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799635033
        }
      },
      "outputs": [],
      "source": [
        "ks['usd_goal_corrected'] = np.log1p(ks['usd_goal'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799636980
        }
      },
      "outputs": [],
      "source": [
        "ks['state'] = ks.state.map(dict(successful=1, failed=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. More data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799640631
        }
      },
      "outputs": [],
      "source": [
        "corr = ks.corr()\n",
        "dims = (16, 10)\n",
        "fig, ax = plt.subplots(figsize = dims)\n",
        "sns.heatmap(corr, \n",
        "            xticklabels=corr.columns.values,\n",
        "            yticklabels=corr.columns.values,ax = ax, cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799642593
        }
      },
      "outputs": [],
      "source": [
        "# We'll drop name_nb_words because it's highly correlated with len_name\n",
        "ks = ks.drop(['name_nb_words'], axis=1)\n",
        "# We can drop currency too as the currency is explained by the country\n",
        "ks = ks.drop(['currency'], axis=1)\n",
        "# We can drop category and main_category as it's encoded in cat_full\n",
        "ks = ks.drop(['category','main_category'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799644253
        }
      },
      "outputs": [],
      "source": [
        "ks.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799645289
        }
      },
      "outputs": [],
      "source": [
        "ks.state.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may consider the dataset is balanced because of the 60/40 % ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799647739
        }
      },
      "outputs": [],
      "source": [
        "ks.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799649368
        }
      },
      "outputs": [],
      "source": [
        "ks.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## II. Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799713936
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ks.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799756048
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ks = pd.get_dummies(ks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799830178
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ks.columns #get_dummies create one-hot encoded columns. every unique category gets a column of 1/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799759701
        }
      },
      "outputs": [],
      "source": [
        "y = ks.state\n",
        "X = ks.drop(['state','usd_goal'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799785966
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Transforming the data\n",
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(scaler.fit_transform(X), columns=list(X.columns))\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799804065
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799806637
        }
      },
      "outputs": [],
      "source": [
        "print('x_train.shape:', X_train.shape)\n",
        "print('y_train.shape:', y_train.shape)\n",
        "print('x_test.shape :', X_test.shape)\n",
        "print('y_test.shape :', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799867530
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Fitting a logistic regression model with default parameters\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799876158
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Making predictions\n",
        "y_hat_train = logreg.predict(X_train)\n",
        "y_hat_test = logreg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799883902
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Logistic regression scores\n",
        "print(\"Logistic regression score for training set:\", round(logreg.score(X_train, y_train),5))\n",
        "print(\"Logistic regression score for test set:\", round(logreg.score(X_test, y_test),5))\n",
        "print(\"\\nClassification report:\")\n",
        "print(classification_report(y_test, y_hat_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799980052
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def plot_cf(y_true, y_pred, class_names=None, model_name=None):\n",
        "    \"\"\"Plots a confusion matrix\"\"\"\n",
        "    cf = confusion_matrix(y_true, y_pred)\n",
        "    plt.imshow(cf, cmap=plt.cm.Blues)\n",
        "    plt.grid(b=None)\n",
        "    if model_name:\n",
        "        plt.title(\"Confusion Matrix: {}\".format(model_name))\n",
        "    else:\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    \n",
        "    class_names = set(y_true)\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    if class_names:\n",
        "        plt.xticks(tick_marks, class_names)\n",
        "        plt.yticks(tick_marks, class_names)\n",
        "    \n",
        "    thresh = cf.max() / 2.\n",
        "    \n",
        "    for i, j in itertools.product(range(cf.shape[0]), range(cf.shape[1])):\n",
        "        plt.text(j, i, cf[i, j], horizontalalignment='center', color='white' if cf[i, j] > thresh else 'black')\n",
        "\n",
        "    plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674799980809
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "plot_cf(y_test, y_hat_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1674800030168
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Plotting the AUC-ROC\n",
        "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
        "\n",
        "print('AUC:', round(auc(fpr, tpr),5))\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.yticks([i/20.0 for i in range(21)])\n",
        "plt.xticks([i/20.0 for i in range(21)])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
